{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sys:1: UserWarning: Your current version of python (3.6.3 |Anaconda custom (64-bit)| (default, Oct  6 2017, 12:04:38) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]) is older then the one used in tests (3.6.4). It is recommended to uprgade it.\n"
     ]
    }
   ],
   "source": [
    "%run ../widgets/config_check.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <api>\n",
    "%matplotlib inline\n",
    "import os\n",
    "path = os.getcwd()\n",
    "s = '/'\n",
    "pardir = s.join(path.split(s)[:-1])\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load widgets\n",
    "from jupyter_cms.loader import load_notebook\n",
    "GD_widgets = load_notebook(str(pardir + '/widgets/2D_gradientdescent_widget.ipynb'))\n",
    "GD_methods = load_notebook(str(pardir + '/widgets/GradientDescent_methods.ipynb'))\n",
    "Widget_targets = load_notebook(str(pardir + '/widgets/Widget_targets.ipynb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variants of gradient descent\n",
    "\n",
    "Vanilla gradient descent slows down to a crawl if the gradient becomes very flat. This can for example be observed on the elliptic normal example. As soon as the elongated vallee is reached, progress along it becomes very slow. Increasing the learning rate helps to some extend, but leads to oscillations of increasing amplitude. The problem being that the gradient and curvature in different directions is of different magnitude. While the learning rate has to be set such that steps are not too large in the steeper/more curved direction thereby slowing the speed of convergence along the shallower direction.\n",
    "\n",
    "Many variants of gradient descent try to remedy this problem by adjusting the gradient steps or learning rate. Here, we will illustrate several well-known methods. Even though we don't prove it here, all these methods are also suitable for stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum\n",
    "\n",
    "The *momentum method* is motivated by a physical analogy: Vanilla gradient can be illustrated by a walker stepping down a hill. At each point, the local direction of steepest descent is found and then a small step in this direction is taken. We could also consider a ball rolling down a hill. In this case, a force acts on the ball in the direction of steepest descent. Without friction, the ball is thereby accelerated and speeds up as long it is rolling down.\n",
    "\n",
    "Momentum consider the update step as $\\mathbf{\\theta}_{n+1} = \\mathbf{\\theta}_n - \\mathbf{v}_{n+1}$ with instantaneous velocity $\\mathbf{v}_{n+1}$. Following the physical picture the velocity is updated as\n",
    "$$ \\mathbf{v}_{n+1} = \\gamma \\mathbf{v}_n + \\lambda \\nabla \\mathcal{L}(\\mathbf{\\theta}_n) . $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<api>\n",
    "class MomentumGD(GD_methods.GradientDescent):\n",
    "    '''Gradient Descent with momentum.'''\n",
    "\n",
    "    def __init__(self, target=Widget_targets.MultNorm(), gamma=0.9, learning_rate=0.1,\n",
    "                num_epochs=20, theta_start=None, stochastic=0):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.velocity = np.zeros_like(theta_start, dtype='float')\n",
    "\n",
    "        super(MomentumGD, self).__init__(target, learning_rate, num_epochs, theta_start, stochastic)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"\\nMomentum gradient descent has performed \"+\\\n",
    "            \"%d steps with %d lieing within the depicted boundaries.\" % (self.num_samples, self.accepted)       \n",
    "            \n",
    "    def comp_update(self, theta):\n",
    "        \n",
    "        grad = self.target_GD.grad(theta)\n",
    "                \n",
    "        update = self.gamma * self.velocity + self.learning_rate * grad\n",
    "        \n",
    "        self.velocity = update\n",
    "        \n",
    "        return update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f50493a3f184ce1b63fe985e7540cdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Figure(axes=[Axis(num_ticks=7, scale=LinearScale(max=3.0, min=-3.0)), Axis(num_ticks=7, orienta…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "widget = GD_widgets.GradientDescentWidget(target='MN2', method='MomentumGDM')\n",
    "widget.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Momentum leads to faster progress along shallow directions as the updates get larger if successive gradients point in the same direction. At the same time, oscillations are dampened as updates gets smaller if the gradient changes sign frequently.\n",
    "\n",
    "### Questions and suggestions for exploration\n",
    "\n",
    "1. What happens if you set $\\gamma = 0$?\n",
    "2. Explore the interplay between $\\gamma$ and the learning rate $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov accelerated gradient descent\n",
    "\n",
    "The momentum method chooses the new speed based on past gradients, i.e. at the current position. Ideally, we would like our ball to slow down already before running up a hill. *Nesterov accelerated gradient descent* adjusts the update step to take into account the gradient at the new position, that is after the update step, to achieve this effect.\n",
    "\n",
    "In formulas, its update step is given by\n",
    "$$ \\begin{align*} \\mathbf{v}_{n+1} &= \\gamma \\mathbf{v}_n + \\lambda \\nabla \\mathcal{L}(\\mathbf{\\theta}_n - \\gamma \\mathbf{v}_n) \\\\\n",
    "   \\mathbf{\\theta}_{n+1} &= \\mathbf{\\theta}_n - \\mathbf{v}_{n+1} \\end{align*}$$\n",
    "where $\\mathbf{\\theta}_n - \\gamma \\mathbf{v}_n$ estimates the new position without taking into account the most recent gradient information.\n",
    "\n",
    "#### Original paper\n",
    "A method for solving the convex programming problem with convergence rate O (1/k^ 2), Yurii Nesterov, Dokl. Akad. Nauk SSSR 269, 543-547"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<api>\n",
    "class NesterovGD(GD_methods.GradientDescent):\n",
    "    '''Nesterov accelerated gradient descent'''\n",
    "    \n",
    "    def __init__(self, target=Widget_targets.MultNorm(), gamma=0.9, learning_rate=0.1,\n",
    "                num_epochs=20, theta_start=None, stochastic=0):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.velocity = np.zeros_like(theta_start, dtype='float')\n",
    "        \n",
    "        super(NesterovGD, self).__init__(target, learning_rate, num_epochs, theta_start, stochastic)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"\\nNesterov gradient descent has performed \"+\\\n",
    "            \"%d steps with %d lieing within the depicted boundaries.\" % (self.num_samples, self.accepted)\n",
    "            \n",
    "    def comp_update(self, theta):\n",
    "        \n",
    "        update = self.gamma * self.velocity + \\\n",
    "                    self.learning_rate * self.target_GD.grad(theta - self.gamma * self.velocity)\n",
    "        \n",
    "        self.velocity = update\n",
    "            \n",
    "        return update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9e41bfc3cd42ffbff3bec02ddb50dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Figure(axes=[Axis(num_ticks=7, scale=LinearScale(max=3.0, min=-3.0)), Axis(num_ticks=7, orienta…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "widget = GD_widgets.GradientDescentWidget(target='MN2', method='NesterovGDM')\n",
    "widget.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced reading suggestion\n",
    "The online journal Distill has published a very nice interactive article called [Why Momentum Really Works](https://distill.pub/2017/momentum/). It delves much deeper into the topic of momentum than the introduction presented here and thus is somewhat more difficult to follow fully. We recommend this material to the advanced and interested reader for more details and an in-depth look at momentum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad\n",
    "\n",
    "The Adagrad method introduces another class of adaptive algorithm which adapt the learning rate for each parameter individually. This provides an alternative way to account for different steepness/curvature in different directions.\n",
    "\n",
    "Adagrad uses the following per parameter update:\n",
    "$$ \\theta_{i,n+1} = \\theta_{i,n} - \\frac{\\lambda}{\\sqrt{g_{i,n}^2 + \\epsilon}} $$\n",
    "Here, $g_{i,n}^2 = \\sum_{k = 1}^n \\left( \\frac{\\partial}{\\partial \\theta_{i,k}} \\mathcal{L}(\\mathbf{\\theta}_k) \\right)^2$ is the sum of the square of all gradients wrt $\\theta_i$ of all previous time steps and $\\epsilon$ prevents division by zero. Thus, the base learning rate is adjusted to the overall size of the gradient of each parameter.\n",
    "\n",
    "Adagrad has been found to be particularly efficient in problems with sparse data. In this case, some parameters are updated frequently -- with small learning rate -- while others are updated only very infrequently -- but with correspondingly larger learning rate.\n",
    "\n",
    "#### Original paper\n",
    "Duchi, J., Hazan, E., & Singer, Y. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 12, 2121–2159.\n",
    "[http://jmlr.org/papers/v12/duchi11a.html](http://jmlr.org/papers/v12/duchi11a.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<api>\n",
    "class ADAGRAD(GD_methods.GradientDescent):\n",
    "    '''Adagrad'''\n",
    "    \n",
    "    def __init__(self, target=Widget_targets.MultNorm(), epsilon=1E-8, learning_rate=0.01,\n",
    "                num_epochs=20, theta_start=None, stochastic=0):\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.past_sq_grad = np.zeros_like(theta_start, dtype='float')\n",
    "        \n",
    "        super(ADAGRAD, self).__init__(target, learning_rate, num_epochs, theta_start, stochastic)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"\\ADAGRAD has performed \"+\\\n",
    "            \"%d steps with %d lieing within the depicted boundaries.\" % (self.num_samples, self.accepted)\n",
    "            \n",
    "    def comp_update(self, theta):\n",
    "        \n",
    "        grad = self.target_GD.grad(theta)\n",
    "        self.past_sq_grad += np.power(grad, 2)\n",
    "        \n",
    "        update = (self.learning_rate/(np.sqrt(self.past_sq_grad) + self.epsilon)) * grad\n",
    "            \n",
    "        return update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394ec8db5dcf4e6e835bf5f1a056866c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Figure(axes=[Axis(num_ticks=7, scale=LinearScale(max=3.0, min=-3.0)), Axis(num_ticks=7, orienta…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "widget = GD_widgets.GradientDescentWidget(target='MN2', method='ADAGRADM')\n",
    "widget.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions and suggestions for exploration\n",
    "\n",
    "1. Explore the influence of the initial learning rate. What differences to vanilla gradient descent do you observe?\n",
    "2. Run Adagrad for a larger number of epochs. What do you observe? Reconsider the update rule and explain your observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp\n",
    "\n",
    "RMSprop is very similar to Adagrad, but instead of summing the squares of all past gradients, it uses a running average, i.e.\n",
    "$$ g_{i,n}^2 = \\gamma g_{i,n-1}^2 + (1 - \\gamma) \\left( \\frac{\\partial}{\\partial \\theta_i} \\mathcal{L}(\\mathbf{\\theta}) \\right)^2 . $$\n",
    "\n",
    "This way, the adaptation to the learning rate depends more strongly on recent gradients with vanish influence of gradients encountered far in the past. Furthermore, the effective learning rate $\\frac{\\lambda}{\\sqrt{g_{i,n}^2 + \\epsilon}}$ can increase again if a shallow region with several small gradients is hit. \n",
    "\n",
    "#### Original paper\n",
    "The RMSProp is unpublished. It was proposed by Geoffrey Hinton in [one of his lectures of his machine learning coursera class](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<api>\n",
    "class RMSProp(GD_methods.GradientDescent):\n",
    "    '''Root Mean Square propagation'''\n",
    "\n",
    "    def __init__(self, target=Widget_targets.MultNorm(), gamma=0.9, epsilon=1E-8, learning_rate=0.1,\n",
    "                num_epochs=20, theta_start=None, stochastic=0):  \n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.avg_sq_grad = np.zeros_like(theta_start, dtype='float')\n",
    "        \n",
    "        super(RMSProp, self).__init__(target, learning_rate, num_epochs, theta_start, stochastic)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"\\nRMSProp has performed \"+\\\n",
    "            \"%d steps with %d lieing within the depicted boundaries.\" % (self.num_samples, self.accepted)       \n",
    "    \n",
    "    def comp_update(self, theta):\n",
    "        \n",
    "        grad = self.target_GD.grad(theta)                    \n",
    "        self.avg_sq_grad = self.gamma * self.avg_sq_grad + (1-self.gamma) * np.power(grad, 2)\n",
    "        \n",
    "        update = (self.learning_rate/(np.sqrt(self.avg_sq_grad) + self.epsilon)) * grad\n",
    "            \n",
    "        return update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7310e633cdb14bc49f4df8629b9077ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Figure(axes=[Axis(num_ticks=7, scale=LinearScale(max=3.0, min=-3.0)), Axis(num_ticks=7, orienta…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "widget = GD_widgets.GradientDescentWidget(target='MN2', method='RMSPropM')\n",
    "widget.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions and suggestions for exploration\n",
    "\n",
    "1. Explore the differences between RMSprop and Adagrad in more detail. Can you observe the effect that the effective learning rate increases during the course of RMSprop?\n",
    "2. Investigate the influence of $\\gamma$. What happens for $\\gamma \\to 0$ and $\\gamma \\to 1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam\n",
    "\n",
    "Adam keeps track of both, past gradients, as well as their magnitudes, squares of past gradients. To this end, two internal variables are updated to compute running averages, i.e.\n",
    "$$ \\begin{align*}\n",
    "     m_{1, n+1} &= \\beta_1 m_{1, n} + (1 - \\beta_1) \\nabla \\mathcal{L}(\\mathbf{\\theta}) \\\\\n",
    "     m_{2, n+1} &= \\beta_2 m_{2, n} + (1 - \\beta_2) \\left( \\nabla \\mathcal{L}(\\mathbf{\\theta}) \\right)^2 \\\\\n",
    "   \\end{align*}\n",
    "$$\n",
    "where the square of the gradient is understood pointwise, i.e. $\\left( \\nabla \\mathcal{L}(\\mathbf{\\theta}) \\right)^2_i = \\left( \\frac{\\partial}{\\partial \\theta_i} \\mathcal{L}(\\mathbf{\\theta}) \\right)^2$.\n",
    "\n",
    "As $m_{1,0} = m_{2,0} = 0$ are initialized at zero, initial gradient estimates are biased towards zero. Adam corrects this bias before applying the estimates in the final learning rate:\n",
    "$$ \\theta_{i,n+1} = \\theta_{i,n} - \\frac{\\lambda}{\\sqrt{\\frac{m_{2,n+1}}{1 - \\beta_2^{n+1}} + \\epsilon}} \\frac{m_{1,n+1}}{1 - \\beta_1^{n+1}} . $$\n",
    "\n",
    "Note that the bias correction gets smaller and smaller for later time steps, i.e. $1 - \\beta_{1,2}^{n+1} \\rightarrow_{n \\to \\infty} 1$, as it should be. In this regard, Adam can be considered as a combination of RMSprop and momentum.\n",
    "\n",
    "#### Original paper\n",
    "Kingma, D. P., & Ba, J. L. (2015). Adam: a Method for Stochastic Optimization. International Conference on Learning Representations (ICLR), 1–13. [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<api>\n",
    "class ADAM(GD_methods.GradientDescent):\n",
    "    '''Adaptive Moment Estimation'''\n",
    "\n",
    "    def __init__(self, target=Widget_targets.MultNorm(), beta1=0.9, beta2=0.999, epsilon=1E-8,\n",
    "                 learning_rate=0.1, num_epochs=20, theta_start=None, stochastic=0):\n",
    "        \n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # initialize moment estimates\n",
    "        self.est_mom_1 = np.zeros_like(theta_start, dtype='float')\n",
    "        self.est_mom_2 = np.zeros_like(theta_start, dtype='float')\n",
    "\n",
    "\n",
    "        super(ADAM, self).__init__(target, learning_rate, num_epochs, theta_start, stochastic)\n",
    "\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"\\nADAM has performed \"+\\\n",
    "            \"%d steps with %d lieing within the depicted boundaries.\" % (self.num_samples, self.accepted)       \n",
    "        \n",
    "    def comp_update(self, theta):\n",
    "        \n",
    "        grad = self.target_GD.grad(theta)\n",
    "                    \n",
    "        self.est_mom_1 = self.beta1 * self.est_mom_1 + (1-self.beta1) * grad\n",
    "        self.est_mom_2 = self.beta2 * self.est_mom_2 + (1-self.beta2) * np.power(grad, 2)\n",
    "            \n",
    "        # bias corrected decaying averages\n",
    "        unbiased_est_mom_1 = self.est_mom_1/(1 - np.power(self.beta1, self.epochID+1))\n",
    "        unbiased_est_mom_2 = self.est_mom_2/(1 - np.power(self.beta2, self.epochID+1))\n",
    "\n",
    "        update = self.learning_rate/(np.sqrt(unbiased_est_mom_2) + self.epsilon) \\\n",
    "                                * unbiased_est_mom_1\n",
    "            \n",
    "        return update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7bbf3b1aaf4c769c3fc01ec44bc8d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Figure(axes=[Axis(num_ticks=7, scale=LinearScale(max=3.0, min=-3.0)), Axis(num_ticks=7, orienta…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "widget = GD_widgets.GradientDescentWidget(target='MN2', method='ADAMM')\n",
    "widget.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, Adam and RMSprop appear to provide a good compromise between robustness -- wrt to the choice of parameters -- and speed of convergence. Both methods are often used in practical applications. Especially, in high-dimensional problems large differences between algorithms can be observed. In this case, many models exhibit loss functions with saddle points. At these points gradients vanish even though they are not local minima, i.e. there are directions along hich the loss increases, as well as directions along which it decreases. A good algorithm should be quickly able to identify the decreasing directions without slowing down due to shallow gradients close to the saddle. \n",
    "\n",
    "### Questions and suggestions for exploration\n",
    "1. Investigate the influence of the parameters $\\beta_1, \\beta_2$ in the Adam method. Explore its behavior for a range of different targets.\n",
    "1. Compare different algorithms around a saddle point. Is there any algorithm which can escape from the saddle point when the inital condition is set at $(\\theta_{1,0}, 0)$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further gradient descent variants and using them in neural networks\n",
    "\n",
    "There is further gradient descent variants that we do not present in more detail as we believe that the above methods present the main concepts. Further methods such as NAdam aim to combine the benefits of e.g. nesterov accelerated gradient descent and the Adam method.  \n",
    "\n",
    "Now that we have introduced gradient descent with its variants and seen how we can find a local minimum (or maximum for gradient ascent) of a function, we can use these algorithms to train neural networks like the [multilayer perceptron (MLP)](GradientDescent_NeuralNetworks_MultilayerPerceptron.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
