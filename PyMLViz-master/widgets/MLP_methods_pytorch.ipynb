{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<api>\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as D\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyter_cms.loader import load_notebook\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <api>\n",
    "import os\n",
    "path = os.getcwd()\n",
    "s = '/'\n",
    "pardir = s.join(path.split(s)[:-1])\n",
    "\n",
    "# Load source notebooks\n",
    "trgt = load_notebook(str(pardir + '/widgets/Widget_targets.ipynb'))\n",
    "widget_methods = load_notebook(str(pardir + '/widgets/Widget_methods.ipynb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<api>\n",
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    Multilayer Perceptron network consisting of an input layer with 2 units, \n",
    "    a number of hidden layers (specified by the user) and an output layer \n",
    "    with one unit. All hidden layers have the same number of units (also \n",
    "    specified by user). All activation functions are non-linear relu functions.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_hidden_layers, n_in_array, n_out_array):\n",
    "        \n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        for l_ID in np.arange(0, n_hidden_layers):\n",
    "            self.layers.append(nn.Linear(n_in_array[l_ID], n_out_array[l_ID], bias=True))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        \n",
    "        self.layers.append(nn.Linear(n_in_array[-1], n_out_array[-1], bias=True))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward pass of network.\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full MLP DrawingMethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<api>\n",
    "class MLP_DrawingMethod(widget_methods.DrawingMethod):\n",
    "    '''\n",
    "    Class to create an MLP network, train the network on given samples,\n",
    "    compute the visualization, importantly implements the draw() method \n",
    "    of the parent classe DrawingMethod.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, target=trgt.MLP_target(), batch_size=1, n_in=2, n_out=1, \n",
    "                 n_hidden_layers=1, n_hidden_units=2, lr=0.01, epochs=1):\n",
    "        '''\n",
    "        Initialization. Receives parameters, initializes network, optimizer \n",
    "        and optimization criterion and samples from the target distribution.\n",
    "        '''    \n",
    "        \n",
    "        self.target = target\n",
    "        self.batch_size = batch_size\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.size = target.get_size()\n",
    "        \n",
    "        # initialize network\n",
    "        self.set_model()\n",
    "        # initialize optimizer\n",
    "        self.set_optim()\n",
    "                \n",
    "        # set loss criterion\n",
    "        self.criterion = nn.MSELoss(reduction='elementwise_mean')\n",
    "       \n",
    "        # get data\n",
    "        data = self.target.get_samples()\n",
    "        self.data_samples = np.empty((len(data['x']), 2))\n",
    "        self.data_samples[:,0] = data['x']\n",
    "        self.data_samples[:,1] = data['y']\n",
    "        self.data_classes = data['z']\n",
    "\n",
    "    def set_dataloader(self, data_samples, data_classes):\n",
    "        '''\n",
    "        Sets the dataloader for the training data.\n",
    "        '''\n",
    "        # create data loader\n",
    "        data_samples = torch.from_numpy(data_samples).float()\n",
    "        data_classes = torch.from_numpy(data_classes).float()\n",
    "\n",
    "        train_dataset = D.TensorDataset(data_samples, data_classes.unsqueeze(1))\n",
    "        \n",
    "        self.trainloader = D.DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def set_viz_dataloader(self):\n",
    "        '''\n",
    "        Sets the dataloader for the meshgrid.\n",
    "        '''\n",
    "        # grid for visualization\n",
    "        self.xx, self.yy = np.meshgrid(np.arange(-self.size, self.size, 0.05),\n",
    "                                         np.arange(-self.size, self.size, 0.05))\n",
    "        self.xx_yy = np.column_stack((self.xx.ravel(), self.yy.ravel()))\n",
    "        \n",
    "        vis_dataset = D.TensorDataset(torch.from_numpy(self.xx_yy).float().unsqueeze(1))\n",
    "        \n",
    "        self.vis_dataloader = D.DataLoader(vis_dataset, batch_size=self.xx_yy.shape[0], shuffle=False)\n",
    "\n",
    "    def set_model(self):\n",
    "        '''\n",
    "        Sets the network architecture.\n",
    "        '''\n",
    "        # array of the number of inputs to each layer (hidden and output layer)\n",
    "        self.n_in_array = np.concatenate(([self.n_in], np.ones(self.n_hidden_layers)\\\n",
    "                                          *self.n_hidden_units)).astype('int')\n",
    "        # array of the number of units in each layer (hidden and output layer)\n",
    "        self.n_out_array = np.concatenate((np.ones(self.n_hidden_layers)\\\n",
    "                                           *self.n_hidden_units, [self.n_out])).astype('int')\n",
    "        self.model = MLP(self.n_hidden_layers, self.n_in_array, self.n_out_array)\n",
    "\n",
    "    def set_optim(self):\n",
    "        '''\n",
    "        Sets the optimizer. Must be implemented by every subclass.\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    def perform_learn(self):\n",
    "        '''\n",
    "        Iterates over training data and optimizes model parameters.\n",
    "        '''\n",
    "        \n",
    "        errors = []\n",
    "        separatrix = []\n",
    "        \n",
    "        for e in range(self.epochs):\n",
    "            for i, (inputs, label) in enumerate(self.trainloader): \n",
    "                self.optim.zero_grad()   # zero the parameter gradients\n",
    "\n",
    "                output = self.model(inputs)\n",
    "\n",
    "                loss = self.criterion(output, label)\n",
    "\n",
    "                loss.backward()\n",
    "                self.optim.step()    # Does the update\n",
    "\n",
    "                errors.append(loss.item())\n",
    "\n",
    "                if i%10 == 0:\n",
    "                    s = self.comp_separatrix()\n",
    "                    separatrix.append(s)\n",
    "                    \n",
    "\n",
    "        s = self.comp_separatrix()\n",
    "        separatrix.append(s)\n",
    "\n",
    "        return errors, separatrix\n",
    "    \n",
    "    def comp_separatrix(self):\n",
    "        '''\n",
    "        Computes the separatrix given the current network state.\n",
    "        '''\n",
    "    \n",
    "        Z = np.array([self.model(inp[0]) for i, inp in enumerate(self.vis_dataloader)])        \n",
    "        Z = np.squeeze(Z[0].detach().numpy())\n",
    "        Z = Z.reshape(self.xx.shape)\n",
    "        return Z\n",
    "    \n",
    "    def draw(self):\n",
    "        '''\n",
    "        Resets the model and optimizer, performs learning the parameters \n",
    "        and computing the separatrix of the optimized model.\n",
    "        '''\n",
    "        # set data loaders for training data and visualization grid\n",
    "        self.set_dataloader(self.data_samples, self.data_classes)\n",
    "        self.set_viz_dataloader()\n",
    "        \n",
    "        # reset model and optimizer\n",
    "        self.set_model()\n",
    "        self.set_optim()\n",
    "        \n",
    "        self.errors, self.separatrix = self.perform_learn()\n",
    "        return {'errors' : self.errors, 'separatrix' : self.separatrix}   \n",
    "    \n",
    "    def set_param(self, param_dict):\n",
    "        '''\n",
    "        Allows to set additional parameters, given from extra_widget.\n",
    "        '''\n",
    "        for i in param_dict:\n",
    "            if i=='layers':\n",
    "                self.n_hidden_layers = param_dict['layers']\n",
    "            elif i=='units':\n",
    "                self.n_hidden_units = param_dict['units']\n",
    "            elif i =='lr':\n",
    "                self.lr = param_dict['lr']\n",
    "            elif i =='epochs':\n",
    "                self.epochs = param_dict['epochs']\n",
    "            elif i =='batch':\n",
    "                self.batch_size = param_dict['batch']\n",
    "            elif i=='target':\n",
    "                self.target = param_dict['target']\n",
    "            elif i=='gamma':\n",
    "                self.gamma = param_dict['gamma']\n",
    "            elif i=='beta1':\n",
    "                self.beta1 = param_dict['beta1']\n",
    "            elif i=='beta2':\n",
    "                self.beta2 = param_dict['beta2']\n",
    "            elif i=='stochastic':\n",
    "                self.stochastic = param_dict['stochastic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various Optimizer DrawingMethods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<api>\n",
    "class MLP_Vanilla_DrawingMethod(MLP_DrawingMethod):\n",
    "    '''\n",
    "    Class for the MLP with the vanilla stochastic gradient descent optimizer.\n",
    "    '''\n",
    "    def set_optim(self):\n",
    "        self.optim = optim.SGD(self.model.parameters(), lr=self.lr, momentum=0., weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<api>\n",
    "class MLP_Momentum_DrawingMethod(MLP_DrawingMethod):\n",
    "    '''\n",
    "    Class for the MLP with the momentum stochastic gradient descent optimizer.\n",
    "    '''\n",
    "    def __init__(self, target=trgt.MLP_target(), batch_size=1, n_in=2, n_out=1, \n",
    "                 n_hidden_layers=1, n_hidden_units=2, lr=0.01, epochs=1, gamma=0.9):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        super(MLP_Momentum_DrawingMethod, self).__init__(target, batch_size, n_in, n_out, \n",
    "                                                         n_hidden_layers, n_hidden_units, lr, epochs)\n",
    "    \n",
    "    def set_optim(self):\n",
    "        self.optim = optim.SGD(self.model.parameters(), lr=self.lr, momentum=self.gamma, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<api>\n",
    "class MLP_Nesterov_DrawingMethod(MLP_DrawingMethod):\n",
    "    '''\n",
    "    Class for the MLP with the Nesterov stochastic gradient descent optimizer.\n",
    "    '''\n",
    "    def __init__(self, target=trgt.MLP_target(), batch_size=1, n_in=2, n_out=1, \n",
    "                 n_hidden_layers=1, n_hidden_units=2, lr=0.01, epochs=1, gamma=0.9):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        super(MLP_Nesterov_DrawingMethod, self).__init__(target, batch_size, n_in, n_out, \n",
    "                                                         n_hidden_layers, n_hidden_units, lr, epochs)\n",
    "    \n",
    "    def set_optim(self):\n",
    "        self.optim = optim.SGD(self.model.parameters(), lr=self.lr, momentum=self.gamma, \n",
    "                               nesterov=True, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<api>\n",
    "class MLP_Adagrad_DrawingMethod(MLP_DrawingMethod):\n",
    "    '''\n",
    "    Class for the MLP with the Adagrad optimizer.\n",
    "    '''\n",
    "    def set_optim(self):\n",
    "        self.optim = optim.Adagrad(self.model.parameters(), lr=self.lr, \n",
    "                                   weight_decay=0, initial_accumulator_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<api>\n",
    "class MLP_RMSProp_DrawingMethod(MLP_DrawingMethod):\n",
    "    '''\n",
    "    Class for the MLP with RMSProp optimizer.\n",
    "    '''\n",
    "    def __init__(self, target=trgt.MLP_target(), batch_size=1, n_in=2, n_out=1, \n",
    "                 n_hidden_layers=1, n_hidden_units=2, lr=0.01, epochs=1, gamma=0.99, eps=1e-08):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        \n",
    "        super(MLP_RMSProp_DrawingMethod, self).__init__(target, batch_size, n_in, n_out, \n",
    "                                                         n_hidden_layers, n_hidden_units, lr, epochs)\n",
    "    \n",
    "    def set_optim(self):\n",
    "        self.optim = optim.RMSprop(self.model.parameters(), lr=self.lr, alpha=self.gamma, eps=self.eps,\n",
    "                                   weight_decay=0, momentum=0, centered=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<api>\n",
    "class MLP_Adam_DrawingMethod(MLP_DrawingMethod):\n",
    "    '''\n",
    "    Class for the MLP with ADAM optimizer.\n",
    "    '''\n",
    "    def __init__(self, target=trgt.MLP_target(), batch_size=1, n_in=2, n_out=1, \n",
    "                 n_hidden_layers=1, n_hidden_units=2, lr=0.01, epochs=1, beta1=0.9, beta2=0.999, eps=1e-08):\n",
    "        \n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        \n",
    "        super(MLP_Adam_DrawingMethod, self).__init__(target, batch_size, n_in, n_out, \n",
    "                                                         n_hidden_layers, n_hidden_units, lr, epochs)\n",
    "    \n",
    "    def set_optim(self):\n",
    "        self.optim = optim.Adam(self.model.parameters(), lr=self.lr, betas=(self.beta1, self.beta1), \n",
    "                                eps=self.eps, weight_decay=0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and train MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = trgt.MLP_target(mean1=[-0.5, 0.5], cov1=[[0.2, 0.],[0., 0.2]], \n",
    "                         mean2=[0.5, -0.5], cov2=[[0.2, 0.],[0., 0.2]],\n",
    "                         n_samples_per_class=100)\n",
    "\n",
    "mlp_dm = MLP_Nesterov_DrawingMethod(target=target, n_in=2, n_out=1, batch_size=1, \n",
    "                                n_hidden_layers=1, n_hidden_units=3, lr=0.005, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_results = mlp_dm.draw()\n",
    "\n",
    "errors = dict_results['errors']\n",
    "separatrix = dict_results['separatrix']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot errors\n",
    "plt.figure()\n",
    "plt.plot(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot separatrix\n",
    "plt.figure()\n",
    "colors = ['orange', 'blue']\n",
    "plt.contourf(mlp_dm.xx, mlp_dm.yy, separatrix[-1], alpha=0.4, \n",
    "                 cmap=matplotlib.colors.ListedColormap(colors))\n",
    "plt.scatter(mlp_dm.data_samples[:, 0], mlp_dm.data_samples[:, 1], c=mlp_dm.data_classes, \n",
    "                cmap=matplotlib.colors.ListedColormap(colors))\n",
    "#plt.xlim([-1,2])\n",
    "#plt.ylim([-1,2])\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
